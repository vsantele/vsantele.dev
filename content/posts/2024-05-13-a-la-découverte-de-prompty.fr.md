---
title: √Ä la d√©couverte de Prompty
date: 2024-05-17T12:00:00.000Z
draft: false
lastmod: 2024-05-17T15:02:23.120Z
description: D√©couvre ce qu'est Prompty, le petit fr√®re de Promptflow et simplifie ton interaction avec des LLMs
series:
  - Promptflow
categories:
  - AI
  - LLM
keywords:
  - AI
  - LLM
  - Promptflow
  - Prompty
type: posts
slug: a-la-decouverte-de-prompty
authors:
  - Victor Santel√©
images:
  - /img/2024/05/superCorgiExtended.webp
---

- [Introduction](#introduction)
- [Mais c'est un peu trop gros pour juste des petits usages non ?](#mais-cest-un-peu-trop-gros-pour-juste-des-petits-usages-non-)
- [C'est cool √ßa, mais c'est d√©j√† utilisable en production ?](#cest-cool-√ßa-mais-cest-d√©j√†-utilisable-en-production-)
- [Et on fait comment pour l'utiliser ?](#et-on-fait-comment-pour-lutiliser-)
  - [Pr√©requis](#pr√©requis)
  - [Cr√©ation de son premier Prompty](#cr√©ation-de-son-premier-prompty)
  - [Connexion avec OpenAi](#connexion-avec-openai)
  - [(Alternative) Connexion avec une API compatible OpenAI](#alternative-connexion-avec-une-api-compatible-openai)
  - [Tester son Prompty](#tester-son-prompty)
  - [Modifier les inputs](#modifier-les-inputs)
  - [Utilisation en Chat](#utilisation-en-chat)
  - [Int√©gration dans un script Python](#int√©gration-dans-un-script-python)
- [Conclusion](#conclusion)

## Introduction

Avant de parler de Prompty, il faut que je te parle un peu de [Promptflow](https://microsoft.github.io/promptflow/?wt.mc_id=studentamb_236461).
C'est une suite d'outil de d√©veloppement pour interagir avec des LLMs facilement pour cr√©er des applis AI.
Le principe de fonctionnement utilise des flows, comme le nom l'indique. On d√©termine une suite d'action √† faire lors d'une requ√™te via un fichier YAML sous forme de DAG combin√© avec du code Python et des fichiers Markdown propuls√© par Jinja.
Il est √©galement possible depuis peu de faire des flows uniquement avec du code Python via des classes ou des fonctions, des flex flow, mais ce n'est pas le sujet d'aujourd'hui.

{{< figure src="/img/2024/05/promptflow-dag.webp" caption="Exemple d'un PromptFlow DAG" >}}

M√™me si c'est un projet d√©velopp√© par Microsoft et disponible sur leur plateforme [AI](https://azure.microsoft.com/fr-fr/solutions/ai?wt.mc_id=studentamb_236461), il est relativement facile de se d√©tacher d'Azure et de le faire tourner o√π on veut via un container docker ou un ex√©cutable.

## Mais c'est un peu trop gros pour juste des petits usages non ?

Oui, c'est vrai, si on veut simplement faire un appel √† OpenAI en utilisant un template simple. Et c'est l√† que Prompty intervient !
Tout se fait dans un fichier qui combine du Markdown et un frontmatter en YAML.
On garde la puissance du moteur de template Jinja avec la simplicit√© de d√©finir sa connexion et ses variables.

Ensuite, on peut utiliser ce fichier avec le CLI de Promptflow ou bien un petit bout de code Python qui est int√©grable o√π on veut.

## C'est cool √ßa, mais c'est d√©j√† utilisable en production ?

Alors, c'est encore en preview, je ferais attention avant de l'envisager en production. La fonctionnalit√© est sortie fin avril, rien ne nous garantit qu'il n'y aura pas de grosses modifications dans le futur.
Cependant, Prompty ne fait rien de magique en soi, il permet simplement de gagner du temps de d√©veloppement pour interagir avec des LLMs.
Si le projet est abandonn√©, ou quoi que ce soit, il sera toujours temps de remplacer le Prompty par du code Python enti√®rement, m√™me si cela prendra un peu de temps √† mettre en place.

## Et on fait comment pour l'utiliser ?

Puisque tu sembles impatient, on va commencer √† rentrer dans le vif du sujet ! Je vais t'expliquer comment cr√©er ton premier Prompty depuis 0 et on va voir pour augmenter la complexit√© (et donc le fun) au fur et √† mesure. Tu es pr√™t ?

### Pr√©requis

Pour commencer, il te faut un acc√®s √† Python. Le site de Promptflow recommandait la version 3.9 jusqu'√† il y a quelques semaines, mais tu peux choisir n'importe quelle version sup√©rieure √† 3.9.
Si tu veux suivre de bonnes pratiques et √©viter de potentielles onflits avec d'autres packages, tu peux cr√©er un environnement virtuel avec venv ou conda

```pwsh
python -m venv prompty
# Linux/MacOS
source prompty/bin/activate
# Windows
prompty\Scripts\activate
```

Ensuite, il faut installer promptflow pour avoir acc√®s au CLI.

```pwsh
pip install promptflow
```

{{< notice warning "Si tu as d√©j√† promptflow" >}}

La version 1.8 de promptflow a modifi√© la structure interne des packages, il est conseill√© de d√©sinstaller les packages et de les r√©installer pour √©viter tout probl√®me.

{{< /notice >}}

Testons si le CLI est bien install√© et est accessible.

```pwsh
pf --version
```

Si tout est bon, tu devrais voir la version des diff√©rents packages de promptflow s'afficher.

### Cr√©ation de son premier Prompty

Maintenant, il est temps de cr√©er son premier fichier `.prompty`. Il s'agit d'un type de fichier reconnu par Promptflow qui combine du Markdown avec un front-matter et le format Jinja.

Cr√©er un fichier `hello-world.prompty` avec le contenu suivant :

```markdown
---
name: "Hello World"
description: "Un simple hello world propuls√© par GPT"
model:
  api: chat
  configuration:
    type: openai
    model:  gpt-3.5-turbo
    api_key: ${env:OPENAI_API_KEY}
  parameters:
    max_tokens: 128
    temperature: 1.4
inputs:
  first_name:
    type: string
  last_name:
    type: string
  tone:
    type: string
sample:
  first_name: John
  last_name: Doe
  tone: c√©r√©monieux
---

system:
Tu es un assistant qui salue les gens. Tu vas ajouter une touche d'originalit√© pour chaque personne. Tu dois avoir un ton "{{tone}}".
Ton seul r√¥le est de saluer les gens. Tu ne dois pas r√©pondre √† des questions ou faire des commentaires sur autre chose.

La personne s'appelle "{{first_name}} {{last_name}}".
```

On va d√©cortiquer un peu le contenu du fichier pour mieux comprendre ce qu'on fait.

Tout d'abord, nous avons le front matter qui respecte la syntaxe YAML. Il est d√©fini entre les deux `---`. Il contient des informations n√©c√©ssaires et par d√©faut pour faire tourner notre Prompty. Voici les diff√©rentes informations que l'on peut y mettre :

- `name`: Le nom de notre Prompty
- `description`: Une description pour savoir ce que fait notre Prompty
- `model`: Les informations pour de connection √† l'API d'openAI ou Azure OpenAI et les param√®tres du mod√®le.
  - `api`: Le type d'API √† utiliser (pour le moment `chat`)
  - `configuration`: Les informations pour se connecter √† l'API d'OpenAI
    - `type`: Pour le moment, seul `openai` et `azure-openai` sont support√©s.
    - `model`: Le nom du mod√®le (openai uniquement)
    - `api_key`: La cl√© d'API pour se connecter
    - `base_url`: Optionnel, l'URL de base pour se connecter √† l'API (openai uniquement).
    - `azure_deployment`: Le nom du d√©ploiement sur Azure OpenAI (azure-openai uniquement)
    - `api_version`: La version de l'API √† utiliser (azure-openai uniquement)
    - `azure_endpoint`: L'URL de l'API (azure-openai uniquement)
  - `parameters`: Les param√®tres pour le mod√®le. Ici, les 2 plus souvent utilis√©s, `max_tokens` et `temperature`.
    - `max_tokens`: Le nombre de tokens √† g√©n√©rer au maximum pour la r√©ponse
    - `temperature`: La temp√©rature pour g√©n√©rer la r√©ponse. Plus la temp√©rature est √©lev√©e, plus la r√©ponse sera "cr√©ative/folle". Plus elle s'approche de 0, plus elle sera reproductible.
  - `inputs`: Les diff√©rentes variables que l'on peut passer √† notre mod√®le. Ici, on a 3 variables, `first_name`, `last_name` et `tone`.
    - `type`: Le type de la variable.
  - `outputs`: Optionnel, les champs de sorties du JSON si on utilise le `response_format`: `json_object`. Il fonctionne comme le champ `inputs`.
  - `sample`: Un exemple de donn√©es qui seront utilis√©es par d√©faut.
  
Ensuite, nous pouvons √©crire notre prompt en Markdown. Le prompt est assez court et se compose que d'un bloc `system`. Il va d√©crire le comportement du mod√®le et lui apporter les informations n√©cessaires pour r√©pondre √† la demande. Ici, on demande simplement au mod√®le de saluer quelqu'un en √©tant original.

Il y a 3 blocs principaux disponibles : `system`, `user` et `assistant`. Le bloc `tool` n'est pas support√© pour le moment. Fais attention √† bien avoir un passage √† la ligne apr√®s le `:` pour que le bloc soit reconnu.

Si tu regardes bien le contenu du markdown, tu peux voir des expressions Jinja. C'est ce qui va permettre d'utiliser les inputs que nous avons d√©finis dans le frontmatter de notre Prompty. Pour uniquement utiliser une variable, il suffit de la mettre entre `{{` et `}}`. Le reste de la syntaxe Jinja est disponible ici : [https://jinja.palletsprojects.com/en/3.1.x/templates/](https://jinja.palletsprojects.com/en/3.1.x/templates/)

### Connexion avec OpenAi

Maintenant que nous avons un prompty il manque les informations de connexion pour l'API d'OpenAI. Tu peux obtenir une cl√© en cliquant [ici](https://platform.openai.com/api-keys?wt.mc_id=studentamb_236461). Tu peux √©galement utiliser une ressource Azure Open AI, mais je ne d√©crirai pas la proc√©dure ici.

Une fois que tu as ta cl√© d'API, tu peux ajouter la variable d'environnement `OPENAI_API_KEY` avec la valeur de ta cl√©.

```pwsh
# Powershell
$env:OPENAI_API_KEY = "ta_cl√©"
# Bash
export OPENAI_API_KEY="ta_cl√©"
```

### (Alternative) Connexion avec une API compatible OpenAI

Si tu ne souhaites pas utiliser OpenAI, ni Azure OpenAI. Tu peux simplement utiliser un autre fournisseur compatible avec l'API d'OpenAI ou utiliser un mod√®le Open Source sur ta machine gr√¢ce √† [ollama](https://ollama.com/) ou [LM Studio](https://lmstudio.ai/) par exemple.

La seule chose √† ajouter dans ton frontmatter est `base_url` dans `model.configuration`. Voici un example pour Ollama:

```yaml
# ... Ta configuration
model:
  api: chat
  configuration:
    type: openai
    model: phi-3 # Non utilis√©
    api_key: "Non utilis√©"
    base_url: "http://localhost:11434/"
# ...
```

Tu peux remplacer `http://localhost:11434/` par `${env:OPENAI_BASE_URL}` pour utiliser une variable d'environnement si tu pr√©f√®res. Cela te permet de facilement changer l'URL sans avoir √† modifier le fichier. Et m√™me d'utiliser OpenAI par la suite avec `https://api.openai.com/v1` comme valeur (Attention √† ton nom de mod√®le).

### Tester son Prompty

Maintenant que tout est en place, il est temps d'obtenir notre premier r√©sultat avec la commande suivante :

```pwsh
pf flow test --flow hello-world.prompty
```

{{< figure src="/img/2024/05/hello-world-prompty.webp" caption="R√©sultat du hello-world" alt="Bonjour, noble John Doe, humblement je vous salue avec toute la c√©r√©monie qui se doit. Puissiez-vous briller comme une √©toile polaire dans le firmament de vos journ√©es." >}}

Si tu cliques sur le lien de la r√©ponse avant la r√©ponse, tu peux voir les √©tapes qu'il y a eues avant d'arriver √† la r√©ponse et obtenir plus de d√©tail. Ce n'est pas vraiment utile pour un Prompty, mais pour un Promptflow plus complexe, cela peut √™tre tr√®s utile.

### Modifier les inputs

Maintenant que le premier test est pass√©, on peut s'amuser avec les variables d'entr√©e via le flag `--inputs`

Par Exemple:

```pwsh
pf flow test --flow hello-world.prompty --inputs first_name=Victor last_name=Santel√© tone=amus√©
```

{{< figure src="/img/2024/05/inputs-prompty.webp" caption="R√©sultat du hello-world avec des inputs" alt="Bonjour Victor Ma√Ætre du Monde Santel√© ! Comment √ßa va ?" >}}

(Promis, j'ai pas trich√© sur le r√©sultat)

### Utilisation en Chat

Maintenant qu'on a un peu jouer avec des entr√©es simples, on va pousser un peu plus loin la logique pour permettre d'int√©ragir avec notre Prompty comme un chat. En gardant l'historique de la conversation en gros.

Il ne faut pas oublier que l'API d'OpenAI (et la majorit√© des LLMs) fonctionne en "stateless", c'est-√†-dire qu'il ne garde pas en m√©moire les pr√©c√©dentes r√©ponses. Pour chaque interaction, il faut envoyer l'ensemble de la conversation dans le contexte pour qu'il puisse r√©pondre correctement.

Pour cela, on va cr√©er un nouveau fichier `chat.prompty` avec le contenu suivant :

```markdown
---
name: "Auteur de mail"
description: "Chatbot pour aider √† la r√©daction d'un mail."
model:
  api: chat
  configuration:
    type: openai
    model:  gpt-3.5-turbo
    api_key: ${env:OPENAI_API_KEY}
  parameters:
    max_tokens: 256
    temperature: 1.4
inputs:
  question:
    type: string
    is_chat_input: true
  chat_history:
    is_chat_history: true
    type: list
    default: []
sample:
  question:
  chat_history: []

---

system:
Tu es un assistant bienveillant. Tu vas aider l'utilisateur a √©crire un mail. Tu as besoin d'un minimum d'information pour commencer: le nom du destinataire, le sujet, un r√©sum√© du contenu et le ton du message. Tu peux demander ces informations √† l'utilisateur en posant des questions. Tu peux aussi donner des conseils pour √©crire un mail efficace. Quand tu as toutes les informations, r√©diges le mail en respectant les demandes de l'utilisateur.

{% for message in chat_history %}
{{ message.author }}:
{{ message.content }}
{% endfor %}
user:
{{ question }}

```

Tu peux ensuite tester le chat avec la commande :

```pwsh
pf flow test --flow chat.prompty --interactive
```

{{< notice tip "Ajoute une inteface web" >}}

Si tu pr√©f√®res une version plus agr√©able √† utiliser pour un chat, rajoute le flag `--ui`

{{< /notice >}}

Qu'est-ce qui a chang√© par rapport √† notre hello world ? D√©j√†, dans les paramtres, on a modifi√© les inputs pour avoir la question de l'utilisateur et l'historique de la conversation.
On a √©galement pr√©cis√© que `question` est une entr√©e de chat avec `is_chat_input: true` et que `chat_history` est un... historique de chat gr√¢ce √† `is_chat_history: true`. Cela permet √† promptflow de comprendre quels champs correspond √† un message dans le chat et o√π stocker l'historique.

Mais, je ne venais pas de dire que les requ√™tes sont stateless?

Et oui, c'est bien le cas! La mention `is_chat_history:true` n'est utile que lors des tests interactifs. C'est une aide pour le d√©veloppeur simplement. En production, il faudra g√©rer nous-m√™mes l'historique de la conversation sous la forme suivante :

```json
[
  { "author": "user", "content": "Bonjour" },
  { "author": "assistant", "content": "Bonjour, comment puis-je vous aider ?" }
]
```

Voici ce que √ßa peut donner :

{{< figure src="/img/2024/05/chat-ui-prompty.webp" caption="Exemple de conversation pour √©crire un mail." alt="Conversation entre le chatbot et un utilisateur qui l'aide √† √©crire un mail." >}}

Si tu es habitu√© √† Azure Ai, tu ne seras pas perdu dans l'interface. Comme on peut le voir dans la capture d'√©cran, il a continu√© de me poser des questions pour obtenir les informations requises pour m'aider √† r√©diger mon e-mail. Gr√¢ce √† l'historique, il a pu garder dans son contexte les informations que je lui ai donn√©es.

### Int√©gration dans un script Python

Maintenant que tu as les bases pour cr√©er un Prompty, tu peux l'int√©grer dans n'importe quel script Python.

Pour cela, il faut charger le fichier `.prompty` depuis la classe `Prompty` de `promptflow.core` et l"utiliser avec les bons `inputs`.

Commence par cr√©er un fichier python `run.py` :

```py
from promptflow.core import Prompty

prompty = Prompty("hello-world.prompty")
response = prompty(first_name="John", last_name="Doe", tone="dr√¥le")
```

{{< figure src="/img/2024/05/python-run-prompty.webp" caption="R√©sultat de l'appel depuis Python" alt="Salut Johnny-boy! Comment √ßa roule aujourd'hui, champion?" >}}

Maintenant tu as les bases pour faire les Prompty que tu souhaites!

## Conclusion

On a pu d√©couvrir pas mal de fonctionnalit√©s de Prompty et Promptflow. J'ai essay√© de rester simple sans oublier de d√©tail pour que √ßa puisse √™tre accessible √† un maximum de monde.

J'esp√®re que je t'ai donn√© l'envie de tester Prompty et de d√©couvrir le reste des fonctionnalit√©s de Promptflow. Je vais publier un autre article bient√¥t pour pr√©senter des fonctionnalit√©s un peu plus avanc√©es et on va finir par ma√Ætriser toutes les possibilit√©s de Promptflow ensemble!

On se revoit vite ! üòé
